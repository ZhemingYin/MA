\chapter{Summary \& Outlook} \label{summary and outlook}
This chapter summarizes the results and improvements of the range-Doppler map upsampling task and give an outlook on the future research.

\section{Summary} \label{summary}
In summary, the thesis uses the combination of multiple data processing methods, models, and loss functions to upsample the range-Doppler map with lower resolution. The range-Doppler map is collected in the indoor environment, that is, it has more information and the dynamic range of the amplitude is wide, which poses the high requirement on the training process. However, through a series of optimizations, the final result is significantly improved compared to the general image upsampling approaches.

Firstly, the motivation of this task is introduced. Although chirp sequence radar can achieve high resolution, it cannot achieve the ideal resolution in practice due to the influence of the whole system or regulation. However, the range-Doppler map is an important part of the subsequent positioning and other operations, so a super-resolution one is needed. Meanwhile, we introduce some state-of-the-art upsampling models and papers.

Since the collection scenarios of many public datasets about the range-Doppler map are not sufficient to cope with complex environment, we choose to collect them by ourselves. Using the radar device of Infineon to dynamically collect data in the corridor, the range-Doppler map will contain both range and velocity information, and can ensure the complexity. Furthermore, by loading the data into the \gls{tfrecord} files to speed up the training process, the data is also processed, such as downsampling, loading it with the certain number of frames, and converting it from the time domain to the frequency domain using \gls{rdp}. Moreover, three methods for preprocessing the data are introduced, namely the input data representation on the complex-valued data in frequency domain, logarithm and normalization operation. According to the comparison and evaluation in section \ref{processing methods comparisons}, we choose the amplitude and phase representation type, logarithm and normalization operations in the range of (0, 1) on the amplitude, but no additional operations are performed on the phase. The resolution and maximum values of the range and velocity are derived as well, which are necessary for the visualization of the range-Doppler map.

According to the image upsampling models in the state of the art introduced above, we built seven models, namely the basic \gls{cnn} model, the basic UNet model, the UNet model with concatenating structure, three Transformer models according to different data division methods, and the \gls{cgan} model. Through the two times comparisons in section \ref{models comparisons}, the \gls{dp}-\gls{tf} Transformer model and SwinIR+DP model look better in evaluation loss and super-resolution range-Doppler maps, especially the \gls{dp}-\gls{tf} Transformer model. The reason is that division along the dimensions of range and velocity is conducive for the Transformer model to better learn the relationship between them. In addition, affected by \gls{rdp}, there are prime numbers in the range dimension, so a variety of processing methods are introduced, among which the padding method was determined according to the evaluation in section \ref{processing methods comparisons}. For the upsampling layer, we also have two methods. In order to reduce the training complexity and ensure the training results, the pixel shuffle approach is selected as the upsampling type. Subsequently, the above optimal solution is combined with \gls{cgan}. According to the results obtained in section \ref{cGAN comparison}, the evaluation losses and the visual effect of the super-resolution range-Doppler map have been optimized to a certain extent through the supervision of the discriminator.

In order to effectively train the upsampling model based on range-Doppler map, a variety of loss functions are used, namely \gls{mse}, \gls{sdr}, \gls{lsd}, \gls{wmse}, \gls{plsd} and \gls{vgg} perceptual loss functions. Through the comparison in section \ref{training loss functions evaluations}, \gls{lsd} achieved a better effect. In order to improve its visual effect, it is combined with perceptual loss, and the ratio is 1:0.5. Furthermore, due to the instability of perceptual loss in the early steps, only \gls{lsd} loss will be used for pretraining in the first 50 epochs. In the subsequent combination with \gls{cgan}, the adversrial loss will be combined. According to the evaluation results in section \ref{cGAN comparison}, 1:0.5:1e-2 is a relatively good loss ratio.

In addition, we also use a series of methods such as distributed training, learning rate schedule, early stopping, and using \gls{wandb} for hyperparameters tuning to improve the training efficiency and final effect of the entire pipeline, which has been verified in section \ref{Distributed training evaluation}, \ref{hyperparameters tuning evaluation} and \ref{final result evaluation}, respectively.

\section{Outlook} \label{outlook}

At the end, by combining multiple data processing methods and loss functions, the \gls{dp}-\gls{tf} Transformer model has made significant progress in upsampling the range-Doppler map in complex indoor environment. However, there are still some possible improvements in the future research.

The content and scale of the currently collected dataset can be further enriched, such as adding more environments. The current data is collected in the corridor of different floors, and the environment is relatively simple. It can be collected in different places such as laboratories, cafes, or subway stations. Although there are other students or scholars passing by in the corridor, areas with more people such as subway stations and cafes will bring more dynamic information. In addition, although there are two types of tempo in the dataset, the overall difference is not large. An alternative is collecting the data while running to make the range-Doppler map get a larger value in the velocity dimension.

For the models based on the Transformer, we can combine the ideas of \gls{swin} Transformer and \gls{dp}-\gls{tf} Transformer blocks. \gls{swin} Transformer divides the range-Doppler map into different small patches and windows, while \gls{dp}-\gls{tf} Transformer block divides the data into a column or a row along the dimensions of range or velocity. Therefore, we can try to divide data into different widths along two dimensions in \gls{dp}-\gls{tf} Transformer block and shift the window between them to see if the results can be improved. Furthermore, since the amplitude has been applied the logarithm and normalization, that is, the range of the amplitude is between 0 and 1, some activation function in the last convolutional layer of the decoder could be used to limit the output.

The selection of many hyperparameters can still be optimized. For example, the relationship between the amplitude loss and the phase loss in each loss function is currently determined based on a rough estimate of the results obtained in the first few batches to ensure that the amplitude accounts for the larger proportion. In further research, different ratios can be tried to improve the selection of this hyperparameter. Meanwhile, although a series of parameters $\lambda_c$ have been compared, more fine-tuning can still be done in the combination ratio between the \gls{lsd} and perceptual loss functions as well as with adversarial loss.

Furthermore, since there are a large number of different settings and combinations in this thesis, in order to make it clear, the evaluation process is currently carried out step by step, that is, the next setting proceed in the case that the last setting is determined, but this may cause some potentially better combinations to be ignored. More combinations could be tried in future research.

In addition, the perceptual loss can be calculated by the features not only obtained by the \gls{vgg} model, but also some other better models or other masking method such as \gls{cfar}. Meanwhile, more loss functions can be chosen to train the model as well.

Last but not least, although many hyperparameters in the model have been tuned, due to resource limit, many parameter ranges are not set very large to prevent the occurrence of out of memory situations. In the future, a relatively larger model or richer alternatives of the hyperparameters could be run on a server with stronger \gls{gpu}s.

